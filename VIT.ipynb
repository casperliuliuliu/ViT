{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74aaa77-40df-4baf-8ad2-1b9f4757c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b05d2d-a2d6-4066-9140-c073df5c8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = \"E:/casper/raw_data_training/test/\"\n",
    "# val_path = \"E:/casper/raw_data_training/val/\"\n",
    "path = \"E:/GLO_ÊûóÂè£Èï∑Â∫öË≥áÊñôÈõÜ/Process_2022/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a8edaf-a42c-4040-9d75-50b9e4d13d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.listdir(train_path + \"MN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980fedcb-bf6e-4550-a1c4-468a18daeebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1754b375364641469c7e028fef00aff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/11399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070d016df37e40fe98f89fb36fb1d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (C:/Users/VLSI/.cache/huggingface/datasets/imagefolder/default-8787100763c9607f/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e03549bb28e4b238209bbdcd344e1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeda7ed-6e54-48a6-a4b2-83fc5b2f8b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 11399\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1376\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590d73dd-5527-4580-bf8e-52114c6b2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96bb3c51-ed1d-4572-9972-5fe3f22627c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets, models, transforms\n",
    "\n",
    "# mean = np.array([0.5, 0.5, 0.5])\n",
    "# std = np.array([0.25, 0.25, 0.25])\n",
    "\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)\n",
    "#     ]),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "203fb265-353a-4de2-981e-9100eb6e9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"E:/casper/raw_data_training/test\"\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "#                                           data_transforms[x])\n",
    "#                   for x in ['train', 'val']}\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "#                                              shuffle=True, num_workers=0)\n",
    "#               for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca52167-f2d5-432c-ac3e-d93baa3a179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 11399\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 1376\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 11399\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 1376\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "prepared_ds = dataset.with_transform(transform)\n",
    "print(prepared_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d8223c9-2fa5-4be0-966e-b0481f872252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = ['IgA','MN']\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5402b763-fa78-4d91-b182-a6442f6ba64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-gggg\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17775408-3b34-4d9b-a140-01a5f82ab33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28d8400-f8aa-4ef2-828f-a580b647947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VLSI\\AppData\\Local\\Temp\\ipykernel_25048\\2425197347.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118792f2-033a-4698-851e-17e578de7324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 11399\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1376\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a24421-da51-46d4-b4d3-2d128c2a3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(\"cuda\"),\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f9d5ab3-2804-4994-b559-5b9d652fa78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VLSI\\anaconda3\\envs\\Tf-GPU\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2852' max='2852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2852/2852 18:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.947689</td>\n",
       "      <td>0.595203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.594100</td>\n",
       "      <td>0.606814</td>\n",
       "      <td>0.676599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.628300</td>\n",
       "      <td>0.673920</td>\n",
       "      <td>0.691134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.570172</td>\n",
       "      <td>0.706395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.569812</td>\n",
       "      <td>0.752907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.582865</td>\n",
       "      <td>0.742006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.516450</td>\n",
       "      <td>0.757994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>0.640440</td>\n",
       "      <td>0.695494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>0.562764</td>\n",
       "      <td>0.723110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.460500</td>\n",
       "      <td>0.577060</td>\n",
       "      <td>0.734738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.545886</td>\n",
       "      <td>0.752180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.640772</td>\n",
       "      <td>0.688227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.522319</td>\n",
       "      <td>0.770349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.545604</td>\n",
       "      <td>0.731105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.528943</td>\n",
       "      <td>0.782703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.546094</td>\n",
       "      <td>0.767442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.539869</td>\n",
       "      <td>0.752180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.217100</td>\n",
       "      <td>0.607769</td>\n",
       "      <td>0.765262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.576032</td>\n",
       "      <td>0.760901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.509730</td>\n",
       "      <td>0.773983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.506966</td>\n",
       "      <td>0.770349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.205700</td>\n",
       "      <td>0.628651</td>\n",
       "      <td>0.776163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.748410</td>\n",
       "      <td>0.766715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.770057</td>\n",
       "      <td>0.755814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.663188</td>\n",
       "      <td>0.768169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.652233</td>\n",
       "      <td>0.773983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.687147</td>\n",
       "      <td>0.776890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>0.672514</td>\n",
       "      <td>0.775436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          4.0\n",
      "  total_flos               = 3290665111GF\n",
      "  train_loss               =       0.3731\n",
      "  train_runtime            =   0:18:08.14\n",
      "  train_samples_per_second =       41.903\n",
      "  train_steps_per_second   =        2.621\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26825ba-6d45-4cda-a458-c26d821ae03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='172' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [172/172 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.7703\n",
      "  eval_loss               =      0.507\n",
      "  eval_runtime            = 0:00:13.00\n",
      "  eval_samples_per_second =     105.78\n",
      "  eval_steps_per_second   =     13.223\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['validation'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e78a93-5b24-44da-8097-a3c0195a451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import smtplib\n",
    "def Notification(subject,message):\n",
    "    content = MIMEMultipart()  \n",
    "    token = \"dpyj vfsd fhhg fhfo\"\n",
    "    email = \"casperliu376@gmail.com\"\n",
    "    content[\"subject\"] = subject\n",
    "    content[\"from\"] = \"VLSI machine\"\n",
    "    content[\"to\"] = \"casperliu1118@gmail.com\"\n",
    "    content.attach(MIMEText(message))\n",
    "    with smtplib.SMTP(host=\"smtp.gmail.com\", port=\"587\") as smtp:\n",
    "        try:\n",
    "            smtp.ehlo()\n",
    "            smtp.starttls()\n",
    "            smtp.login(email, token)\n",
    "            smtp.send_message(content)\n",
    "            print(\"Complete!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error message: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d8bcdbc-683a-4430-8070-1e050b67b009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "mes = \"\"\"HI CASPER,\n",
    "\n",
    "    I AM TRANSFORMER, PLEASE CONTACT ME.\n",
    "\n",
    "SINCERELY,\n",
    "VLSI\n",
    "\"\"\"\n",
    "sub = \"VIT\"\n",
    "Notification(sub,mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d43af5b6-5a51-4943-8e65-497cbbc54503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5701d3d-65d6-4e8f-886d-63c58eeef0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 32451.7074, 'train_samples_per_second': 1.405, 'train_steps_per_second': 0.088, 'total_flos': 3.533324758466937e+18, 'train_loss': 0.36472809202394346, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "print(train_results.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tf-GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
